{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "912beb6a",
   "metadata": {},
   "source": [
    "# VectorDB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f95d49",
   "metadata": {},
   "source": [
    "| DB | Description |\n",
    "|----|-------------|\n",
    "| Regulation DB | íšŒì‚¬ ë‚´ê·œ ë˜ëŠ” ë²•ë¥ ì— ëŒ€í•œ VectorDB |\n",
    "| Space DB | ì‚¬ì˜¥ ë‚´ ì •ë³´, ì¸ì‚¬ ì¡°ì§ë„ ë° ì—°ë½ì²˜ ë“± ì •ë³´ì— ëŒ€í•œ VectorDB |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8284c5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc63d0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7990eeb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "# ğŸ“Œ VectorDB íŒŒì´í”„ë¼ì¸ (Jupyter Notebook í…ŒìŠ¤íŠ¸ìš©)\n",
    "# - PDF í…ìŠ¤íŠ¸ + ì´ë¯¸ì§€ Embedding\n",
    "# - FAISS ê¸°ë°˜ VectorDB êµ¬ì¶•\n",
    "# - ë¬¸ì„œ ëª©ë¡ ì¡°íšŒ / ì‚­ì œ ê¸°ëŠ¥ í¬í•¨\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple\n",
    "from io import BytesIO\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import fitz  # PyMuPDF\n",
    "import faiss\n",
    "\n",
    "from langchain.vectorstores import FAISS as LangChainFAISS\n",
    "from langchain.schema import Document\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.docstore.in_memory import InMemoryDocstore\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# -----------------------------------------\n",
    "# âœ… ì„¤ì •\n",
    "# -----------------------------------------\n",
    "\n",
    "DATA_DIRS = {\n",
    "    \"regulation\": {\n",
    "        \"raw\": \"./data/raw/regulation\",\n",
    "        \"processed\": \"./data/processed/regulation\",\n",
    "        \"embeddings\": \"./data/embeddings/regulation\"\n",
    "    },\n",
    "    \"space\": {\n",
    "        \"raw\": \"./data/raw/space\",\n",
    "        \"processed\": \"./data/processed/space\",\n",
    "        \"embeddings\": \"./data/embeddings/space\"\n",
    "    }\n",
    "}\n",
    "\n",
    "VECTORDB_DIRS = {\n",
    "    \"regulation\": \"./vectordb/regulation_db\",\n",
    "    \"space\": \"./vectordb/space_db\"\n",
    "}\n",
    "\n",
    "EMBEDDING_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "\n",
    "# ë¬¸ì¥ ì„ë² ë”©ìš© ëª¨ë¸ (ë¹ ë¥¸ ì‚¬ìš©ì„ ìœ„í•´ ë¯¸ë¦¬ ë¡œë“œ)\n",
    "model = SentenceTransformer(EMBEDDING_MODEL)\n",
    "\n",
    "# -----------------------------------------\n",
    "# âœ… PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ\n",
    "# -----------------------------------------\n",
    "\n",
    "def extract_text_from_pdf(pdf_path: str) -> str:\n",
    "    doc = fitz.open(pdf_path)\n",
    "    texts = []\n",
    "    for page in doc:\n",
    "        texts.append(page.get_text())\n",
    "    return \"\\n\".join(texts)\n",
    "\n",
    "# -----------------------------------------\n",
    "# âœ… PDF ì´ë¯¸ì§€ ì¶”ì¶œ\n",
    "# -----------------------------------------\n",
    "\n",
    "def extract_images_from_pdf(pdf_path: str) -> List[Image.Image]:\n",
    "    doc = fitz.open(pdf_path)\n",
    "    images = []\n",
    "    for page in doc:\n",
    "        for img_index, img in enumerate(page.get_images(full=True)):\n",
    "            xref = img[0]\n",
    "            base_image = doc.extract_image(xref)\n",
    "            image = Image.open(BytesIO(base_image[\"image\"]))\n",
    "            images.append(image)\n",
    "    return images\n",
    "\n",
    "# -----------------------------------------\n",
    "# âœ… ì´ë¯¸ì§€ â†’ ì„ë² ë”© ë³€í™˜\n",
    "# -----------------------------------------\n",
    "\n",
    "def image_to_embedding(image: Image.Image) -> np.ndarray:\n",
    "    image = image.resize((64, 64)).convert(\"RGB\")\n",
    "    arr = np.array(image).flatten() / 255.0\n",
    "    return arr[:384] if arr.size >= 384 else np.pad(arr, (0, 384 - arr.size))\n",
    "\n",
    "# -----------------------------------------\n",
    "# âœ… VectorDB êµ¬ì¶•\n",
    "# -----------------------------------------\n",
    "\n",
    "def build_vectordb(target: str):\n",
    "    data_info = DATA_DIRS[target]\n",
    "    vectordb_dir = VECTORDB_DIRS[target]\n",
    "\n",
    "    os.makedirs(data_info[\"processed\"], exist_ok=True)\n",
    "    os.makedirs(data_info[\"embeddings\"], exist_ok=True)\n",
    "    os.makedirs(vectordb_dir, exist_ok=True)\n",
    "\n",
    "    index = faiss.IndexFlatL2(384)\n",
    "    docs = []\n",
    "\n",
    "    embedding_model = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL)\n",
    "\n",
    "    files = list(Path(data_info[\"raw\"]).glob(\"*.pdf\"))\n",
    "\n",
    "    if len(files) == 0:\n",
    "        print(f\"[ê²½ê³ ] {data_info['raw']} ê²½ë¡œì— PDF íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        return\n",
    "\n",
    "    for file in tqdm(files, desc=f\"Processing {target} PDFs\"):\n",
    "\n",
    "        text = extract_text_from_pdf(str(file))\n",
    "        text_file = Path(data_info[\"processed\"]) / (file.stem + \".txt\")\n",
    "        text_file.write_text(text, encoding=\"utf-8\")\n",
    "\n",
    "        text_embedding = embedding_model.embed_query(text)\n",
    "        emb_path = Path(data_info[\"embeddings\"]) / (file.stem + \"_text.npy\")\n",
    "        np.save(emb_path, text_embedding)\n",
    "\n",
    "        index.add(np.array([text_embedding], dtype=np.float32))\n",
    "        docs.append(Document(page_content=text, metadata={\"source\": str(file), \"type\": \"text\"}))\n",
    "\n",
    "        images = extract_images_from_pdf(str(file))\n",
    "        for idx, img in enumerate(tqdm(images, desc=f\"{file.stem} ì´ë¯¸ì§€ ì²˜ë¦¬\")):\n",
    "            img_embedding = image_to_embedding(img)\n",
    "            emb_path = Path(data_info[\"embeddings\"]) / f\"{file.stem}_img{idx}.npy\"\n",
    "            np.save(emb_path, img_embedding)\n",
    "\n",
    "            index.add(np.array([img_embedding], dtype=np.float32))\n",
    "            docs.append(Document(page_content=f\"Image from {file.stem} (index {idx})\", metadata={\"source\": str(file), \"type\": f\"image_{idx}\"}))\n",
    "\n",
    "    faiss.write_index(index, os.path.join(vectordb_dir, \"index.faiss\"))\n",
    "    index_to_docstore_id = {i: str(i) for i in range(len(docs))}\n",
    "    docstore = InMemoryDocstore({str(i): doc for i, doc in enumerate(docs)})\n",
    "\n",
    "    lc_faiss = LangChainFAISS(embedding_model, index, docstore, index_to_docstore_id)\n",
    "    lc_faiss.save_local(vectordb_dir)\n",
    "\n",
    "    print(f\"âœ… Saved VectorDB to {vectordb_dir}\")\n",
    "\n",
    "# -----------------------------------------\n",
    "# âœ… ë¬¸ì„œ ì¶”ê°€\n",
    "# -----------------------------------------\n",
    "\n",
    "def add_document_to_vectordb(pdf_path: str, target: str, security_level: str = \"ì¤‘\"):\n",
    "    vectordb_dir = VECTORDB_DIRS[target]\n",
    "    embedding_model = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL)\n",
    "    vectordb = LangChainFAISS.load_local(vectordb_dir, embedding_model, allow_dangerous_deserialization=True)\n",
    "\n",
    "    text = extract_text_from_pdf(pdf_path)\n",
    "    text_embedding = embedding_model.embed_query(text)\n",
    "\n",
    "    text_doc = Document(\n",
    "        page_content=text,\n",
    "        metadata={\"source\": pdf_path, \"type\": \"text\", \"security_level\": security_level}\n",
    "    )\n",
    "\n",
    "    vectordb.index.add(np.array([text_embedding], dtype=np.float32))\n",
    "    new_docstore_id = str(len(vectordb.docstore._dict))\n",
    "    vectordb.docstore._dict[new_docstore_id] = text_doc\n",
    "    vectordb.index_to_docstore_id[vectordb.index.ntotal - 1] = new_docstore_id\n",
    "\n",
    "    images = extract_images_from_pdf(pdf_path)\n",
    "    for idx, img in enumerate(tqdm(images, desc=f\"{Path(pdf_path).stem} ì´ë¯¸ì§€ ì¶”ê°€\")):\n",
    "        img_embedding = image_to_embedding(img)\n",
    "        img_doc = Document(\n",
    "            page_content=f\"Image from {Path(pdf_path).stem} (index {idx})\",\n",
    "            metadata={\"source\": pdf_path, \"type\": f\"image_{idx}\", \"security_level\": security_level}\n",
    "        )\n",
    "\n",
    "        vectordb.index.add(np.array([img_embedding], dtype=np.float32))\n",
    "        new_docstore_id = str(len(vectordb.docstore._dict))\n",
    "        vectordb.docstore._dict[new_docstore_id] = img_doc\n",
    "        vectordb.index_to_docstore_id[vectordb.index.ntotal - 1] = new_docstore_id\n",
    "\n",
    "    vectordb.save_local(vectordb_dir)\n",
    "\n",
    "    print(f\"âœ… {pdf_path} ì¶”ê°€ ì™„ë£Œ (ë³´ì•ˆë“±ê¸‰: {security_level})\")\n",
    "\n",
    "# -----------------------------------------\n",
    "# âœ… ë¬¸ì„œ ëª©ë¡ ì¡°íšŒ\n",
    "# -----------------------------------------\n",
    "\n",
    "def list_vectordb(vectordb_dir: str) -> List[Tuple[str, str]]:\n",
    "    \"\"\"\n",
    "    VectorDB ë‚´ ë¬¸ì„œ ëª©ë¡ì„ ì¡°íšŒí•©ë‹ˆë‹¤. (íŒŒì¼ëª… + ë³´ì•ˆë“±ê¸‰ë§Œ ë°˜í™˜, ì¤‘ë³µ ì œê±°)\n",
    "\n",
    "    Returns:\n",
    "        List of (íŒŒì¼ëª…, ë³´ì•ˆë“±ê¸‰)\n",
    "    \"\"\"\n",
    "    embedding_model = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL)\n",
    "    vectordb = LangChainFAISS.load_local(vectordb_dir, embedding_model, allow_dangerous_deserialization=True)\n",
    "\n",
    "    docs = vectordb.docstore._dict.values()\n",
    "\n",
    "    summary = {}\n",
    "    for doc in docs:\n",
    "        source = doc.metadata.get(\"source\", \"unknown\")\n",
    "        security_level = doc.metadata.get(\"security_level\", \"ì¤‘\")\n",
    "\n",
    "        file_name = Path(source).name\n",
    "        summary[file_name] = security_level  # ê°™ì€ íŒŒì¼ëª…ì´ë©´ ë®ì–´ì”€ (ë³´ì•ˆë“±ê¸‰ì€ ë™ì¼í•¨)\n",
    "\n",
    "    return list(summary.items())\n",
    "\n",
    "# -----------------------------------------\n",
    "# âœ… íŠ¹ì • ë¬¸ì„œ ì‚­ì œ\n",
    "# -----------------------------------------\n",
    "\n",
    "def delete_document_from_vectordb(target: str, doc_source_name: str):\n",
    "    vectordb_dir = VECTORDB_DIRS[target]\n",
    "    embedding_model = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL)\n",
    "    vectordb = LangChainFAISS.load_local(vectordb_dir, embedding_model, allow_dangerous_deserialization=True)\n",
    "\n",
    "    remaining_docs = []\n",
    "    delete_docs = []\n",
    "\n",
    "    for doc in vectordb.docstore._dict.values():\n",
    "        source = doc.metadata.get(\"source\", \"\")\n",
    "        if doc_source_name in source:\n",
    "            delete_docs.append(doc)\n",
    "        else:\n",
    "            remaining_docs.append(doc)\n",
    "\n",
    "    if not delete_docs:\n",
    "        print(f\"â— ì‚­ì œ ëŒ€ìƒ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {doc_source_name}\")\n",
    "        return\n",
    "\n",
    "    print(f\"âœ… {len(delete_docs)}ê°œ ë¬¸ì„œ ì‚­ì œ\")\n",
    "\n",
    "    new_index = faiss.IndexFlatL2(384)\n",
    "    new_docstore = {}\n",
    "    new_index_to_docstore_id = {}\n",
    "\n",
    "    for i, doc in enumerate(remaining_docs):\n",
    "        content = doc.page_content\n",
    "        doc_type = doc.metadata.get(\"type\", \"unknown\")\n",
    "\n",
    "        if doc_type.startswith(\"image\"):\n",
    "            arr = image_to_embedding(Image.new(\"RGB\", (64, 64)))\n",
    "        else:\n",
    "            arr = embedding_model.embed_query(content)\n",
    "\n",
    "        new_index.add(np.array([arr], dtype=np.float32))\n",
    "        new_docstore[str(i)] = doc\n",
    "        new_index_to_docstore_id[new_index.ntotal - 1] = str(i)\n",
    "\n",
    "    new_vectordb = LangChainFAISS(embedding_model, new_index, InMemoryDocstore(new_docstore), new_index_to_docstore_id)\n",
    "    new_vectordb.save_local(vectordb_dir)\n",
    "\n",
    "    print(f\"âœ… VectorDB ì¬êµ¬ì„± ì™„ë£Œ ({vectordb_dir})\")\n",
    "\n",
    "# -----------------------------------------\n",
    "# âœ… VectorDB ì „ì²´ ì‚­ì œ\n",
    "# -----------------------------------------\n",
    "\n",
    "def delete_vectordb(vectordb_dir: str):\n",
    "    if os.path.exists(vectordb_dir):\n",
    "        shutil.rmtree(vectordb_dir)\n",
    "        print(f\"âœ… {vectordb_dir} ì „ì²´ ì‚­ì œ ì™„ë£Œ\")\n",
    "    else:\n",
    "        print(\"â— ì‚­ì œí•  VectorDBê°€ ì—†ìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "080d4ad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Step 1] VectorDB êµ¬ì¶• í…ŒìŠ¤íŠ¸\n",
      "VectorDBê°€ ì—†ê±°ë‚˜ index.faissê°€ ì—†ìœ¼ë¯€ë¡œ ìƒˆë¡œ êµ¬ì¶•í•©ë‹ˆë‹¤.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_document ì´ë¯¸ì§€ ì²˜ë¦¬: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10842/10842 [00:16<00:00, 650.91it/s]\n",
      "ê·¼ë¡œê¸°ì¤€ë²•(ë²•ë¥ )(ì œ20520í˜¸)(20250223) ì´ë¯¸ì§€ ì²˜ë¦¬: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 142.89it/s]\n",
      "Processing regulation PDFs: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:27<00:00, 13.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved VectorDB to ./vectordb/regulation_db\n"
     ]
    }
   ],
   "source": [
    "def test_vectordb_flow():\n",
    "    target = \"regulation\"  # í…ŒìŠ¤íŠ¸í•  target ì„¤ì •\n",
    "    vectordb_dir = VECTORDB_DIRS.get(target)\n",
    "    index_file = os.path.join(vectordb_dir, \"index.faiss\")\n",
    "\n",
    "    if vectordb_dir is None:\n",
    "        print(f\"â— ëŒ€ìƒ '{target}' ì´(ê°€) VECTORDB_DIRSì— ë“±ë¡ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.\")\n",
    "        return\n",
    "\n",
    "    print(\"\\n[Step 1] VectorDB êµ¬ì¶• í…ŒìŠ¤íŠ¸\")\n",
    "    if not os.path.exists(index_file):\n",
    "        print(\"VectorDBê°€ ì—†ê±°ë‚˜ index.faissê°€ ì—†ìœ¼ë¯€ë¡œ ìƒˆë¡œ êµ¬ì¶•í•©ë‹ˆë‹¤.\")\n",
    "        build_vectordb(target)\n",
    "    else:\n",
    "        print(\"âœ… ì´ë¯¸ êµ¬ì¶•ëœ VectorDBê°€ ì¡´ì¬í•©ë‹ˆë‹¤.\")\n",
    "\n",
    "    # print(\"\\n[Step 2] ë¬¸ì„œ ì¶”ê°€ í…ŒìŠ¤íŠ¸ (ë³´ì•ˆë“±ê¸‰ ìƒìœ¼ë¡œ ì¶”ê°€)\")\n",
    "    # test_pdf_path = \"./data/raw/regulation/test_document.pdf\"\n",
    "\n",
    "    # if not os.path.exists(test_pdf_path):\n",
    "    #     print(\"â— í…ŒìŠ¤íŠ¸ìš© PDF íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ê²½ë¡œ: './data/raw/regulation/test_document.pdf'\")\n",
    "    #     return\n",
    "\n",
    "    # add_document_to_vectordb(test_pdf_path, target, security_level=\"ìƒ\")\n",
    "\n",
    "    # print(\"\\n[Step 3] VectorDB ë¬¸ì„œ ëª©ë¡ ì¡°íšŒ (ë³´ì•ˆë“±ê¸‰ í™•ì¸)\")\n",
    "    # entries = list_vectordb(vectordb_dir)\n",
    "    # if not entries:\n",
    "    #     print(\"â— VectorDBì— ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "    # else:\n",
    "    #     for file_name, security_level in entries:\n",
    "    #         print(f\"{file_name} | ë³´ì•ˆë“±ê¸‰: {security_level}\")\n",
    "\n",
    "    # print(\"\\n[Step 4] íŠ¹ì • ë¬¸ì„œ ì‚­ì œ í…ŒìŠ¤íŠ¸ (test_document.pdf)\")\n",
    "    # delete_document_from_vectordb(target, \"test_document.pdf\")\n",
    "\n",
    "    # print(\"\\n[Step 5] VectorDB ë¬¸ì„œ ëª©ë¡ ì¬ì¡°íšŒ (ì‚­ì œ ê²°ê³¼ í™•ì¸)\")\n",
    "    # entries_after_delete = list_vectordb(vectordb_dir)\n",
    "    # if not entries_after_delete:\n",
    "    #     print(\"âœ… ëª¨ë“  ë¬¸ì„œê°€ ì‚­ì œë˜ì–´ VectorDBê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.\")\n",
    "    # else:\n",
    "    #     for file_name, security_level in entries_after_delete:\n",
    "    #         print(f\"{file_name} | ë³´ì•ˆë“±ê¸‰: {security_level}\")\n",
    "\n",
    "    # print(\"\\n[Step 6] VectorDB ì „ì²´ ì‚­ì œ í…ŒìŠ¤íŠ¸\")\n",
    "    # delete_vectordb(vectordb_dir)\n",
    "\n",
    "    # if not os.path.exists(vectordb_dir):\n",
    "    #     print(\"âœ… VectorDB ì „ì²´ ì‚­ì œ ì™„ë£Œ\")\n",
    "    # else:\n",
    "    #     print(\"â— VectorDB ì „ì²´ ì‚­ì œ ì‹¤íŒ¨\")\n",
    "        \n",
    "test_vectordb_flow()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce04bf4a",
   "metadata": {},
   "source": [
    "# Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28993dd8",
   "metadata": {},
   "source": [
    "| Agent | Description | DB |\n",
    "|-------|-------------|----|\n",
    "| Regulation Agent | íšŒì‚¬ ë‚´ê·œ ë˜ëŠ” ë²•ë¥ ì— ëŒ€í•´ ë‹µë³€í•˜ëŠ” Agentë¡œ, Regulation DB ê¸°ë°˜ RAG í˜•ì‹ìœ¼ë¡œ êµ¬ì¶• | Regulation DB |\n",
    "| Space Agent | ì‚¬ì˜¥ ë‚´ ì •ë³´, ì¸ì‚¬ ì¡°ì§ë„ ë° ì—°ë½ì²˜ ë“± ì§ˆë¬¸ì— ëŒ€í•´ ë‹µë³€í•˜ëŠ” Agentë¡œ, Space DB ê¸°ë°˜ RAG í˜•ì‹ìœ¼ë¡œ êµ¬ì¶• | Space DB |\n",
    "| Worker Agent | ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•˜ì—¬ ì´ì „ Reference ìë£Œ ë˜ëŠ” ì–‘ì‹(Template)ì„ ì°¾ì•„ì£¼ëŠ” Agentë¡œ, ì‚¬ë‚´ DB APIë¥¼ í˜¸ì¶œí•˜ì—¬ ë‹µë³€ | ì‚¬ë‚´ DB API |\n",
    "| General Agent | ìœ„ ë‚´ìš© ì™¸ì˜ ì¼ë°˜ì ì¸ ì§ˆë¬¸(ex. ë‚ ì”¨ ë“±)ì— ëŒ€í•´ ë‹µë³€í•˜ëŠ” Agent | ì™¸ë¶€ ì¼ë°˜ ë°ì´í„° |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad89c5f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26a060e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… API Key ì¸ì¦ ì„±ê³µ! ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸:\n",
      "- gpt-4o-audio-preview-2024-12-17\n",
      "- dall-e-3\n",
      "- dall-e-2\n",
      "- gpt-4o-audio-preview-2024-10-01\n",
      "- gpt-4-turbo-preview\n"
     ]
    }
   ],
   "source": [
    "client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "try:\n",
    "    models = client.models.list()\n",
    "    print(\"âœ… API Key ì¸ì¦ ì„±ê³µ! ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸:\")\n",
    "    for model in models.data[:5]:\n",
    "        print(\"-\", model.id)\n",
    "except Exception as e:\n",
    "    print(\"âŒ API Key ì¸ì¦ ì‹¤íŒ¨:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "97d20a79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Supervisor ìµœì¢… Intent]: space\n",
      "\n",
      "=== ìµœì¢… ì‘ë‹µ ===\n",
      "ì‚¬ë‚´ ì¹´í˜ëŠ” ì§€ìƒ 1ì¸µì— ìœ„ì¹˜í•´ ìˆìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langgraph.graph import StateGraph, END, START\n",
    "from typing import TypedDict, Optional, Literal\n",
    "from openai import OpenAI\n",
    "import os\n",
    "\n",
    "# Load environment\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "# print(\"OpenAI API Key:\", OPENAI_API_KEY)\n",
    "\n",
    "# -----------------------\n",
    "# VectorDB ë¡œë“œ (FAISS)\n",
    "# -----------------------\n",
    "\n",
    "EMBEDDING_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "embeddings = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL)\n",
    "\n",
    "regulation_vectordb = FAISS.load_local(\"./vectordb/regulation_db\", embeddings, index_name=\"index\", allow_dangerous_deserialization=True)\n",
    "space_vectordb = FAISS.load_local(\"./vectordb/space_db\", embeddings, index_name=\"index\", allow_dangerous_deserialization=True)\n",
    "\n",
    "# -----------------------\n",
    "# Company API Client\n",
    "# -----------------------\n",
    "\n",
    "class CompanyAPIClient:\n",
    "    def __init__(self):\n",
    "        self.templates = {\n",
    "            \"íšŒì˜ë¡\": \"íšŒì˜ë¡ ì–‘ì‹: - íšŒì˜ ì¼ì‹œ - ì¥ì†Œ - ì°¸ì„ì - ì£¼ìš” ì•ˆê±´ - ë…¼ì˜ ë‚´ìš© - ê²°ë¡  ë° ì¡°ì¹˜ ì‚¬í•­\",\n",
    "            \"ì¶œì¥ ë³´ê³ ì„œ\": \"ì¶œì¥ ë³´ê³ ì„œ ì–‘ì‹: - ì¶œì¥ì§€ - ì¶œì¥ ê¸°ê°„ - ì¶œì¥ ëª©ì  - ì£¼ìš” í™œë™ - ê²°ê³¼ ë° í–¥í›„ ê³„íš\",\n",
    "            \"ê²½ì¡°ì‚¬ ì‹ ì²­\": \"ê²½ì¡°ì‚¬ ì‹ ì²­ì„œ ì–‘ì‹: - ì‹ ì²­ì¸ - ê´€ê³„ - ê²½ì¡°ì‚¬ ì¢…ë¥˜ - ì¼ì‹œ ë° ì¥ì†Œ - ê¸°íƒ€ ì‚¬í•­\",\n",
    "        }\n",
    "\n",
    "    def search_templates(self, query: str) -> Optional[str]:\n",
    "        for keyword, template in self.templates.items():\n",
    "            if keyword in query:\n",
    "                return template\n",
    "        return None\n",
    "\n",
    "api_client = CompanyAPIClient()\n",
    "\n",
    "# -----------------------\n",
    "# LLM ì´ˆê¸°í™”\n",
    "# -----------------------\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "# -----------------------\n",
    "# Agents ì •ì˜\n",
    "# -----------------------\n",
    "\n",
    "def regulation_agent(user_input: str) -> str:\n",
    "    docs = regulation_vectordb.similarity_search(user_input, k=3)\n",
    "    context = \"\\n\".join([d.page_content for d in docs])\n",
    "    prompt = f\"ë‚´ê·œ ë¬¸ì„œ:\\n{context}\\n\\nì§ˆë¬¸: {user_input}\"\n",
    "    return llm.invoke(prompt).content\n",
    "\n",
    "def space_agent(user_input: str) -> str:\n",
    "    docs = space_vectordb.similarity_search(user_input, k=3)\n",
    "    context = \"\\n\".join([d.page_content for d in docs])\n",
    "    prompt = f\"ì‚¬ì˜¥ ë¬¸ì„œ:\\n{context}\\n\\nì§ˆë¬¸: {user_input}\"\n",
    "    return llm.invoke(prompt).content\n",
    "\n",
    "def worker_agent(user_input: str) -> str:\n",
    "    template = api_client.search_templates(user_input)\n",
    "    if template:\n",
    "        return f\"Template ì¶”ì²œ:\\n\\n{template}\"\n",
    "    else:\n",
    "        return llm.invoke(f\"ì ì ˆí•œ ì–‘ì‹ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ëŒ€ì‹  ì¼ë°˜ ê°€ì´ë“œë¥¼ ì‘ì„±í•˜ì„¸ìš”.\\n\\n{user_input}\").content\n",
    "\n",
    "def general_agent(user_input: str) -> str:\n",
    "    return llm.invoke(user_input).content\n",
    "\n",
    "# -----------------------\n",
    "# Agent Mapping\n",
    "# -----------------------\n",
    "\n",
    "AGENT_MAP = {\n",
    "    \"regulation\": regulation_agent,\n",
    "    \"space\": space_agent,\n",
    "    \"worker\": worker_agent,\n",
    "    \"general\": general_agent,\n",
    "}\n",
    "\n",
    "# -----------------------\n",
    "# LangGraph ìƒíƒœ ì •ì˜\n",
    "# -----------------------\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    user_input: str\n",
    "    intent: Optional[Literal[\"regulation\", \"space\", \"worker\", \"general\"]]\n",
    "    response: Optional[str]\n",
    "    next_node: Optional[str]\n",
    "\n",
    "# -----------------------\n",
    "# Hybrid Supervisor (VectorDB + LLM)\n",
    "# -----------------------\n",
    "\n",
    "def supervisor_agent(state: AgentState) -> AgentState:\n",
    "    user_input = state[\"user_input\"]\n",
    "\n",
    "    system_prompt = \"\"\"\n",
    "                    ë„ˆëŠ” ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë‹¤ìŒ 4ê°œ Intent ì¤‘ í•˜ë‚˜ë¡œ ë¶„ë¥˜í•˜ëŠ” ì—ì´ì „íŠ¸ì´ë‹¤.\n",
    "\n",
    "                    Intent ì¢…ë¥˜:\n",
    "\n",
    "                    - regulation: íšŒì‚¬ ë‚´ê·œ, ë²•ë¥ , ì¸ì‚¬, ê·¼íƒœ, ì—°ì°¨, íœ´ê°€, ê²½ì¡°ì‚¬ ë“± ê·œì •ì´ë‚˜ ì •ì±…ì— ê´€í•œ ì§ˆë¬¸\n",
    "                    - space: ì‚¬ì˜¥, ì¡°ì§ë„, ì—°ë½ì²˜, ì¢Œì„, ìœ„ì¹˜, ì‹œì„¤, íšŒì˜ì‹¤ ë“± ê³µê°„ê³¼ ê´€ë ¨ëœ ì§ˆë¬¸\n",
    "                    - worker: ì—…ë¬´ ì²˜ë¦¬ ë°©ë²•, ì—…ë¬´ ì–‘ì‹, í…œí”Œë¦¿ ë“± í˜•ì‹ì´ë‚˜ ì ˆì°¨ì™€ ê´€ë ¨ëœ ì§ˆë¬¸\n",
    "                    - general: ìœ„ì— í•´ë‹¹í•˜ì§€ ì•ŠëŠ” ì¼ë°˜ì ì¸ ì§ˆë¬¸ (ì˜ˆ: ë‚ ì”¨, ì¼ìƒ ëŒ€í™”, íšŒì‚¬ì™€ ê´€ë ¨ ì—†ëŠ” ì§ˆë¬¸ ë“±)\n",
    "\n",
    "                    ê·œì¹™:\n",
    "\n",
    "                    1. ë°˜ë“œì‹œ ìœ„ 4ê°œ ì¤‘ í•˜ë‚˜ë§Œ ì†Œë¬¸ìë¡œ ì •í™•íˆ ê³¨ë¼ì„œ ë‹µë³€í•˜ë¼.\n",
    "                    2. ì§ˆë¬¸ì´ ê·œì •ì´ë‚˜ ì¸ì‚¬ ê´€ë ¨ì´ë©´ regulation ìœ¼ë¡œ ë¶„ë¥˜í•˜ë¼.\n",
    "                    3. ì§ˆë¬¸ì´ ì‚¬ì˜¥ì´ë‚˜ ê³µê°„ ì •ë³´ ê´€ë ¨ì´ë©´ space ìœ¼ë¡œ ë¶„ë¥˜í•˜ë¼.\n",
    "                    4. ì§ˆë¬¸ì´ ì—…ë¬´ ì–‘ì‹ì´ë‚˜ ì ˆì°¨ ê´€ë ¨ì´ë©´ worker ë¡œ ë¶„ë¥˜í•˜ë¼.\n",
    "                    5. ì–´ë–¤ ê²ƒë„ ì•„ë‹ˆê±°ë‚˜ ì¼ìƒ/ì¼ë°˜ì  ì§ˆë¬¸ì´ë©´ general ë¡œ ë¶„ë¥˜í•˜ë¼.\n",
    "                    6. ê·¸ ì™¸ëŠ” ì ˆëŒ€ ë‹¤ë¥¸ ë‹¨ì–´ë¥¼ ì‚¬ìš©í•˜ì§€ ë§ê³  ìœ„ 4ê°œ ì¤‘ í•˜ë‚˜ë¡œë§Œ ë‹µí•˜ë¼.\n",
    "\n",
    "                    ì§ˆë¬¸: {input}\n",
    "                    ë‹µë³€ (intent ì¤‘ í•˜ë‚˜ë§Œ): \n",
    "                    \"\"\"\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\")\n",
    "    ])\n",
    "\n",
    "    chain = prompt | llm | (lambda x: x.content.strip().lower())\n",
    "    intent = chain.invoke({\"input\": user_input})\n",
    "\n",
    "    print(f\"[Supervisor ìµœì¢… Intent]: {intent}\")\n",
    "\n",
    "    return {\n",
    "        **state,\n",
    "        \"intent\": intent,\n",
    "        \"next_node\": intent\n",
    "    }\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# ê³µí†µ Agent Executor\n",
    "# -----------------------\n",
    "\n",
    "def common_agent_executor(state: AgentState) -> AgentState:\n",
    "    agent_name = state[\"next_node\"]\n",
    "    user_input = state[\"user_input\"]\n",
    "\n",
    "    agent = AGENT_MAP.get(agent_name)\n",
    "\n",
    "    if agent is None:\n",
    "        result = \"ì ì ˆí•œ Agentê°€ ì—†ìŠµë‹ˆë‹¤.\"\n",
    "    else:\n",
    "        result = agent(user_input)\n",
    "\n",
    "    return {**state, \"response\": result}\n",
    "\n",
    "# -----------------------\n",
    "# Graph êµ¬ì„± (START â†’ Supervisor â†’ Executor â†’ END)\n",
    "# -----------------------\n",
    "\n",
    "graph = StateGraph(AgentState)\n",
    "\n",
    "graph.add_node(\"supervisor\", supervisor_agent)\n",
    "graph.add_node(\"agent_executor\", common_agent_executor)\n",
    "\n",
    "graph.add_edge(START, \"supervisor\")\n",
    "graph.add_conditional_edges(\"supervisor\", lambda state: state[\"next_node\"], {\n",
    "    \"regulation\": \"agent_executor\",\n",
    "    \"space\": \"agent_executor\",\n",
    "    \"worker\": \"agent_executor\",\n",
    "    \"general\": \"agent_executor\",\n",
    "})\n",
    "graph.add_edge(\"agent_executor\", END)\n",
    "\n",
    "runnable = graph.compile()\n",
    "\n",
    "# -----------------------\n",
    "# í…ŒìŠ¤íŠ¸ ì‹¤í–‰\n",
    "# -----------------------\n",
    "\n",
    "inputs = {\"user_input\": \"ì‚¬ë‚´ ì¹´í˜ ëª‡ ì¸µì— ìˆë‚˜ìš”?\"}\n",
    "\n",
    "result = runnable.invoke(inputs)\n",
    "\n",
    "print(\"\\n=== ìµœì¢… ì‘ë‹µ ===\")\n",
    "print(result[\"response\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
